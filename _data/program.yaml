page-title: <strong>P</strong>rogram

#
#program-title: Program
#program-description: |
#    Last updated on March 13th
#    <br/>
#
#    <a href="../img/print/AHs2022Program.pdf" class="btn btn-md" style="background-color: #FFC847; border: 2px solid #F4FDD9; color: black !important; margin-top: 25px;" onMouseOver="this.style['background-color']='#E05263'" onMouseOut="this.style['background-color']='#FFC847'">Download PDF</a>

program-title: Program Schedule
program: This year‚Äôs Augmented Humans Conference takes place over 5 days to facilitate the programme. Each conference day ends at 14:00 to be in accordance with University working hours during Ramadan.

# awards-title: Awards
# awards: |
#   <h3 style="color: #FFC847;">üèÜ Best Paper Award</h3>
#   <strong>Real-time Slow-motion : A Framework for Slow-motion Without Deviating from Real-Time</strong>
#   <p class="authors"> Goki Muramoto, Hiroto Saito, Sohei Wakisaka and Masahiko Inami</p>
#   <img src="../img/bestpaper.png" alt="bestpapaer" style="width:23em;">
#   </br>
#   <h3 style="color: #FFC847;">üèÜ Best Paper Honourable Mention </h3>
#   <strong>Serendipity Wall: A Discussion Support System Using Real-time Speech Recognition and Large Language Model</strong>
#   <p class="authors">Shota Imamura, Hirotaka Hiraki and Jun Rekimoto.</p>
#   <img src="../img/hm.png" alt="Honourable" style="width:23em;">
#   </br>
#   <h3 style="color: #FFC847;">üèÜ Best Poster Award</h3>
#   <strong>Aged Eyes: Optically Simulating Presbyopia Using Tunable Lenses</strong>
#   <p class="authors">Qing Zhang, Yoshihito Kondoh, Yuta Itoh and Jun Rekimoto</p>
#   </br>
#   <h3 style="color: #FFC847;">üèÜ Best Demo Award</h3>
#   <strong>Demonstrating VabricBeads</strong>
#   <p class="authors">Jefferson Pardomuan, Shio Miyafuji, Nobuhiro Takahashi and Hideki Koike</p>
#   <img src="../img/demo.png" alt="demo" style="width:23em;">


session-title: Session Overview
sessions:
  - session: "Session 1: Wearables and sensors"
    papers:
    - title: 'WhisperMask: a noise suppressive mask-type microphone for whisper speech'
      authors: "Hirotaka Hiraki, Shusuke Kanazawa, Takahiro Miura, Manabu Yoshida, Masaaki Mochimaru and Jun Rekimoto"
      # doi: 'https://doi.org/10.1145/3582700.3582728'
      time: 'April 4th 11:00 - 11:15'
    - title: 'Synthetic Visual Sensations: Augmenting Human Spatial Awareness with a Wearable Retinal Electric Stimulation Device'
      authors: 'Valdemar Munch Danry, Laura Chicos, Matheus Fonseca, Ishraki Kazi and Pattie Maes'
      # doi: 'https://doi.org/10.1145/3582700.3582704'
      time: 'April 4th 11:15 - 11:30'
    - title: 'Looking From a Different Angle: Placing Head-Worn Displays Near the Nose'
      authors: 'Yukun Song, Parth Arora, Srikanth T. Varadharajan, Rajandeep Singh, Malcolm Haynes and Thad Starner'
      # doi: 'https://doi.org/10.1145/3582700.3582711'
      time: 'April 4th 11:30 - 11:45'
    # - title: 'Tactile Vectors for Omnidirectional Arm Guidance'
    #   authors: 'Hesham Elsayed et al.'
      # doi: 'https://doi.org/10.1145/3582700.3582701'
      # time: '11:45 - 12:00'
    - title: 'Future So Bright, Gotta Wear Shades: Lens Tint May Affect Social Perception of Head-Worn Displays'
      authors: 'Sofia Vempala, Joseph Mushyakov, Srikanth Tindivanam Varadharajan and Thad Starner'
      # doi: 'https://doi.org/10.1145/3582700.3582713'
      time: 'April 4th 11:45 - 12:00'
    # - title: 'Q&A'
    #   time: '12:45 - 13:00'

  - session: "Session 2: Extended Reality"
    papers:
    - title: 'Everyday Life Challenges and Augmented Realities: Exploring Use-Cases For, and User Perspectives on, an Augmented Everyday Life'
      authors: 'Florian Mathis'
      # doi: 'https://doi.org/10.1145/3582700.3582709'
      time: 'April 4th 14:00 - 14:15'
    - title: 'GestureMark: Shortcut Input Technique using Smartwatch Touch Gestures for XR Glasses'
      authors: 'Juyoung Lee, Minju Baeck, Hui-Shyong Yeo, Thad Starner and Woontack Woo'
      # doi: 'https://doi.org/10.1145/3582700.3582712'
      time: 'April 4th 14:15 - 14:30'
    - title: 'Holistic Patient Assessment System using Digital Twin for XR Medical Teleconsultation'
      authors: 'Taeyeon Kim, Hyunsong Kwon, Kyunghyun Cho and Woontack Woo'
      # doi: 'https://doi.org/10.1145/3582700.3582705'
      time: 'April 4th 14:30 - 14:45'
    - title: 'Exploring the Kuroko Paradigm: The Effect of Enhancing Virtual Humans with Reality Actuators in Augmented Reality'
      authors: '√âmilie Fabre and Yuta Itoh'
      # doi: 'https://doi.org/10.1145/3582700.3582703'
      time: 'April 4th 14:45 - 15:00'
    - title: 'Real-time Slow-motion : A Framework for Slow-motion Without Deviating from Real-Time'
      authors: 'Goki Muramoto, Hiroto Saito, Sohei Wakisaka and Masahiko Inami'
      # doi: 'https://doi.org/10.1145/3582700.3582717'
      time: 'April 4th 15:00 - 15:15'
    # - title: 'Tablet Cutting Board: Tablet-based Knife-control Support System for Cookery Beginners'
    #   authors: 'Hatsune Masuda et al.'
    #   doi: 'https://doi.org/10.1145/3582700.3582708'
    #   time: '15:25 - 15:35'
    # - title: 'Challenges in Virtual Reality Studies: Ethics and Internal and External Validity'
    #   authors: 'Sarah Delgado Rodriguez et al.'
    #   doi: 'https://doi.org/10.1145/3582700.3582716'
    #   time: '15:35 - 15:45'
    # - title: 'Q&A'
    #   time: '15:45 - 16:00'

  - session: "Session 3: Wearables and sensors 2"
    papers:
    - title: 'Identifying Hand-based Input Preference Based on Wearable EEG'
      authors: 'Kaining Zhang, Zehong Cao, Xianglin Zheng and Mark Billinghurst'
      # doi: 'https://doi.org/10.1145/3582700.3582720'
      time: 'April 4th 16:30 - 16:45'
    - title: 'Personal Identification and Authentication Method Using Ear Images Acquired with a Camera-Equipped Hearable Device'
      authors: 'Yurina Mizuho, Yohei Kawasaki, Takashi Amesaka and Yuta Sugiura'
      # doi: 'https://doi.org/10.1145/3582700.3582714'
      time: 'April 4th 16:45 - 17:00'
    - title: 'iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing'
      authors: 'Mengxi Liu, Hymalai Bello, Bo Zhou, Paul Lukowicz and Jakob Karolus'
      # doi: 'https://doi.org/10.1145/3582700.3582715'
      time: 'April 4th 17:00 - 17:15'
    - title: 'Auditory Interface for Empathetic Synchronization of Facial Expressions between People with Visual Impairment and the Interlocutors'
      authors: 'Takayuki Komoda, Hisham Elser Bilal Salih, Tadashi Ebihara, Naoto Wakatsuki and Keiichi Zempo'
      # doi: 'https://doi.org/10.1145/3582700.3582702'
      time: 'April 4th 17:15 - 17:30'
    # - title: 'DecluttAR: An Interactive Visual Clutter Dimming System to Help Focus on Work'
    #   authors: 'Kaito Yokoro et al.'
    #   doi: 'https://doi.org/10.1145/3582700.3582718'
    #   time: '10:00 - 10:15'
    # - title: 'ShadowClones: an Interface to Maintain a Multiple Sense of Body-space Coordination in Multiple Visual Perspectives'
    #   authors: 'Kazuma Takada et al.'
    #   doi: 'https://doi.org/10.1145/3582700.3582706'
    #   time: '10:15 - 10:30'
    # - title: 'Q&A'
    #   time: '10:30 - 10:45'


  # - session: "Session 4: Augmenting in VR"
  #   papers:
  #   - title: 'Techniques using Parallel Views for Asynchronous VR Search Tasks'
  #     authors: 'Theophilus Teo et al.'
  #     # doi: 'https://doi.org/10.1145/3582700.3582720'
  #     time: '9:00 - 9:15'
  #   - title: 'Social Simon Effect in Virtual Reality: Investigating the Impact of Co-actor Avatar's Visual Representation'
  #     authors: 'Xiaotong Li et al.'
  #     # doi: 'https://doi.org/10.1145/3582700.3582714'
  #     time: '9:15 - 9:30'
  #   - title: 'Multiplexed VR: Individualized Multiplexing Virtual Environment to Facilitate Switches for Group Ideation Creativity'
  #     authors: 'Masahiro Kamihira et al.'
  #     # doi: 'https://doi.org/10.1145/3582700.3582715'
  #     time: '9:30 - 9:45'
  #   - title: 'VR remote tourism system with natural Gaze induction without causing user discomfort'
  #     authors: 'Shogo Aoyagi et al.'
  #     # doi: 'https://doi.org/10.1145/3582700.3582702'
  #     time: '9:45 - 10:00'
  #   - title: 'GUI Presentation Method based on Binocular Rivalry for Non-overlay Information Recognition in Visual Scenes'
  #     authors: 'Kai Guo et al.'
  #     # doi: 'https://doi.org/10.1145/3582700.3582702'
  #     time: '9:45 - 10:00'


  - session: "Session 4: Augmenting in VR"
    papers:
    - title: 'Techniques using Parallel Views for Asynchronous VR Search Tasks'
      authors: 'Theophilus Teo, Kuniharu Sakurada, Maki Sugimoto, Gun Lee and Mark Billinghurst'
      time: 'April 5th 9:00 - 09:15'
    - title: 'Social Simon Effect in Virtual Reality: Investigating the Impact of Coactor Avatars Visual Representation'
      authors: 'Xiaotong Li, Yuji Hatada and Takuji Narumi'
      time: 'April 5th 09:15 - 09:30'
    - title: 'Multiplexed VR: Individualized Multiplexing Virtual Environment to Facilitate Switches for Group Ideation Creativity'
      authors: 'Masahiro Kamihira, Juro Hosoi, Yuki Ban and Shin Ichi Warisawa'
      time: 'April 5th 09:30 - 09:45'
    - title: 'VR remote tourism system with natural Gaze induction without causing user discomfort'
      authors: 'Shogo Aoyagi, Takayoshi Yamada, Kelvin Cheng, Soh Masuko and Keiichi Zempo'
      time: 'April 5th 09:45 - 10:00'
    - title: 'GUI Presentation Method based on Binocular Rivalry for Non-overlay Information Recognition in Visual Scenes'
      authors: 'Kai Guo, Yuki Shimomura, Juro Hosoi, Yuki Ban and Shin Ichi Warisawa'
      time: 'April 5th 10:00 - 10:15'


  - session: "Session 5: Learning Augmentation"
    papers:
    - title: 'FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts'
      authors: 'Kazuki Kawamura and Jun Rekimoto'
      # doi: 'https://doi.org/10.1145/3582700.3582727'
      time: 'April 5th 14:00 - 14:15'
    - title: 'SkillsInterpreter: A Case Study of Automatic Annotation of Flowcharts to Support Browsing Instructional Videos in Modern Martial Arts using Large Language Models'
      authors: 'Kotaro Oomori, Yoshio Ishiguro and Jun Rekimoto'
      # doi: 'https://doi.org/10.1145/3582700.3582719'
      time: 'April 5th 14:15 - 14:30'
    - title: 'Kavy: Fostering Language Speaking Skills and Self-Confidence Through Conversational AI'
      authors: 'Sankha Cooray, Chathuranga Hettiarachchi, Vishaka Nanayakkara, Denys Matthies, Yasith Samaradivakara and Suranga Nanayakkara'
      # doi: 'https://doi.org/10.1145/3582700.3582726'
      time: 'April 5th 14:30 - 14:45'
    - title: 'Serendipity Wall: A Discussion Support System Using Real-time Speech Recognition and Large Language Model'
      authors: 'Shota Imamura, Hirotaka Hiraki and Jun Rekimoto'
      # doi: 'https://doi.org/10.1145/3582700.3582723'
      time: 'April 5th 14:45 - 15:00'
    # - title: 'Human Coincident Robot: A Non-contact Surrounding Robot Sharing the Coordinate with a Human Inside'
    #   authors: 'Takafumi Watanabe et al.'
    #   doi: 'https://doi.org/10.1145/3582700.3582724'
    #   time: '15:15 - 15:30'
    # - title: 'Q&A'
    #   time: '15:30 - 15:45'



# session-title: Session Schedules
# session: |
#   <h3 style="color: black;">Session 1: Haptic Feedback</h3>
#   <table id="sessions">
#   <tr>
#     <th>Time</th>
#     <th>Paper</th> 
#   </tr>
#   <tr>
#     <td>11:30 - 11:45</td>
#     <td>
#     S78: Designing Interactive Shoes for Tactile Augmented Reality
#     <p class="authors">Test</p>
#     <p class="doi"></p>
#     </td> 
#   </tr>
#   <tr>
#     <td>11:45 - 12:00</td>
#     <td>S15: GAuze-MIcrosuture-FICATION: Gamification in Microsuture training with real-time feedback</td> 
#   </tr>
#   <tr>
#     <td>12:00 - 12:15</td>
#     <td>S32: Standing Balance Improved by Electrical Muscle Stimulation to Popliteus Muscles</td> 
#   </tr>
#   <tr>
#     <td>12:15 - 12:30</td>
#     <td>S5: Tactile Vectors for Omnidirectional Arm Guidance</td> 
#   </tr>
#   <tr>
#     <td>12:30 - 12:45</td>
#     <td>S38: Coldness Presentation to Ventral Forearm using Electrical Stimulation with Elastic Gel and Anesthetic Cream</td> 
#   </tr>
#   <tr>
#     <td>12:45 - 13:00</td>
#     <td>Q&A</td> 
#   </tr>
#   </table>

#   <h3 style="color: black;">Session 2: Perception and Extended Reality</h3>
#   <table id="sessions">
#   <tr>
#     <th>Time</th>
#     <th>Paper</th> 
#   </tr>
#   <tr>
#     <td>14:15 - 14:30</td>
#     <td>S30: Virtual Omnibus Lecture: Investigating the Effects of Varying Lecturer Avatars as Environmental Context on Audience Memory</td> 
#   </tr>
#   <tr>
#     <td>14:30 - 14:45</td>
#     <td>S36: LocatAR: An AR Object Search Assistance System for a Shared Space</td> 
#   </tr>
#   <tr>
#     <td>14:45 - 15:00</td>
#     <td>S16: Generation of realistic facial animation of a CG avatar speaking a moraic language</td> 
#   </tr>
#   <tr>
#     <td>15:00 - 15:15</td>
#     <td>S12: Effect of Weight Adjustment in Virtual Co-embodiment During Collaborative Training</td> 
#   </tr>
#   <tr>
#     <td>15:15 - 15:25</td>
#     <td>S43: Workspace Scaling in Virtual Reality based Robot Teleoperation</td> 
#   </tr>
#   <tr>
#     <td>15:25 - 15:35</td>
#     <td>S22: Tablet Cutting Board: Tablet-based Knife-control Learning Support System for Cookery Beginners</td> 
#   </tr>
#   <tr>
#     <td>15:35 - 15:45</td>
#     <td>S42: Challenges in Virtual Reality Studies: Ethics and Internal and External Validity</td> 
#   </tr>
#   <tr>
#     <td>15:45 - 16:00</td>
#     <td>Q&A</td> 
#   </tr>
#   </table>

#   <h3 style="color: black;">Session 3: Human Empowerment</h3>
#   <table id="sessions">
#   <tr>
#     <th>Time</th>
#     <th>Paper</th> 
#   </tr>
#   <tr>
#     <td>9:00 - 9:15</td>
#     <td>S58: Effects of Wearing Knee-tightening Devices and Presenting Shear Forces to the Knee on Redirected Walking</td> 
#   </tr>
#   <tr>
#     <td>9:15 - 9:30</td>
#     <td>S39: LUNAChair: Remote Wheelchair System Linking Users to Nearby People and Assistants</td> 
#   </tr>
#   <tr>
#     <td>9:30 - 9:45</td>
#     <td>S40: Visuospatial abilities and cervical spine range of motion improvement effects of a non-goal-oriented VR travel program at an older adults facility:A pilot randomized controlled trial</td> 
#   </tr>
#   <tr>
#     <td>9:45 - 10:00</td>
#     <td>S8: Exploration of Sonification Feedback for People with Visual Impairment to Use Ski Simulator</td> 
#   </tr>
#   <tr>
#     <td>10:00 - 10:15</td>
#     <td>S44: DecluttAR: An Interactive Visual Clutter Dimming System to Help Focus on Work</td> 
#   </tr>
#   <tr>
#     <td>10:15 - 10:30</td>
#     <td>S18: ShadowClones: an Interface to Maintain a Multiple Sense of Body-space Coordination in Multiple Visual Perspectives</td> 
#   </tr>
#   <tr>
#     <td>10:30 - 10:45</td>
#     <td>Q&A</td> 
#   </tr>
#   </table>

#   <h3 style="color: black;">Session 4: Intelligence Augmentation</h3>
#   <table id="sessions">
#   <tr>
#     <th>Time</th>
#     <th>Paper</th> 
#   </tr>
#   <tr>
#     <td>11:30 - 11:45</td>
#     <td>S31: AI Coach: A Motor Skill Training System using Motion Discrepancy Detection</td> 
#   </tr>
#   <tr>
#     <td>11:45 - 12:00</td>
#     <td>S20: CC-Glasses: Color Communication Support for People with Color Vision Deficiency Using Augmented Reality and Deep Learning</td> 
#   </tr>
#   <tr>
#     <td>12:00 - 12:15</td>
#     <td>S64: AIx speed: Playback Speed Optimization using Listening Comprehension of Speech Recognition Models</td> 
#   </tr>
#   <tr>
#     <td>12:15 - 12:30</td>
#     <td>S70: Exoskeleton for the Mind: Exploring Strategies Against Misinformation with a Metacognitive Agent</td> 
#   </tr>
#   <tr>
#     <td>12:30 - 12:45</td>
#     <td>S63: Investigating Effects of Facial Self-Similarity Levels on the Impression of Virtual Agents in Serious/Non-Serious Contexts</td> 
#   </tr>
#   <tr>
#     <td>12:45 - 13:00</td>
#     <td>Q&A</td> 
#   </tr>
#   </table>

#   <h3 style="color: black;">Session 5: Interfaces for the Body and Beyond</h3>
#   <table id="sessions">
#   <tr>
#     <th>Time</th>
#     <th>Paper</th> 
#   </tr>
#   <tr>
#     <td>14:15 - 14:30</td>
#     <td>S74: Affective Umbrella -- A Wearable System to Visualize Heart and Electrodermal Activity, towards Emotion Regulation through Somaesthetic Appreciation</td> 
#   </tr>
#   <tr>
#     <td>14:30 - 14:45</td>
#     <td>S53: TOMURA: A Mountable Hand-shaped Interface for Versatile Interactions</td> 
#   </tr>
#   <tr>
#     <td>14:45 - 15:00</td>
#     <td>S71: DUMask: A Discrete and Unobtrusive Mask-Based Interface for Facial Gestures</td> 
#   </tr>
#   <tr>
#     <td>15:00 - 15:15</td>
#     <td>S67: Dynamic Derm: Body Surface Deformation Display for Real-World Embodied Interactions</td> 
#   </tr>
#   <tr>
#     <td>15:15 - 15:30</td>
#     <td>S68: Human Coincident Robot: A Non-contact Surrounding Robot Sharing the Coordinate with a Human Inside</td> 
#   </tr>
#   <tr>
#     <td>15:30 - 15:45</td>
#     <td>Q&A</td> 
#   </tr>
#   </table>

keynote-title: 'Opening Keynote'
keynote: |
  <h3 style="color:#FFC847;">Skill Acquisition and Transfer Systems with AI and XR for Augmented Human</h3>
  <strong>Abstract:</strong> As AI has advanced significantly and begun to surpass human capabilities, the importance of human augmentation technologies to enhance human abilities has increased. Among these, skill acquisition support not only streamlines the acquisition of advanced skills by experts in various fields such as sports and music, but also contributes to the pursuit of well-being through hobby acquisition for the general public. In this presentation, we will introduce the skill acquisition support system that we are currently researching and developing. By incorporating the latest AI and XR technologies, we aim to liberate users from complex measurement environments, enable AI to explain core skills, and provide real-time intuitive feedback. Furthermore, I will discuss the ecosystem made possible by the democratization of skill acquisition support systems.
keynote-author: |
  <strong> Prof. Hideki Koike</strong>
  <strong>Short Bio:</strong>Hideki Koike is a professor at the School of Computing, Tokyo Institute of Technology, Japan. He received his B.E., M.E., and Dr. Eng. degrees from the University of Tokyo in 1986, 1988, and 1991, respectively. After working at the University of Electro-Communications, Tokyo, he joined the Tokyo Institute of Technology in 2014. His research interests include vision-based human-computer interaction, human augmentation, information visualization, and usable security.
# keynote-short: |
#   <h3 style="color:#003865;">Behavior and Physiology-Aware Security User Interfaces</h3>
# keynote-author-short: |
#   <strong>Florian Alt</strong> is a full professor of Usable Security and Privacy at the Research Institute for Cyber Defence (CODE) in Munich.
keynote-img: /img/keynote-speaker/hideki.jpg


closekeynote-title: 'Closing Keynote'
closekeynote: |
  <h3 style="color:#003865;">Augmented Astronaut Survival</h3>
  <strong>Abstract:</strong>As humanity confronts the escalating complexities of space exploration, the development of innovative methodologies tailored to augment humans in the lunar environment is essential. This keynote posits a transformative framework for experimental approaches in space survival, particularly for the Artemis missions, through creative engagement and inventive brainstorming. It takes a participatory "research-through-performance," scenario approach to probe the intricate tapestry of scientific, engineering, and cultural challenges that future lunar missions will inevitably face. The early NASA Moon Survival Task, initiated before the Apollo landings, provided insights into team-think dynamics and decision-making under duress, ensuring its relevance and use for over 50 years now. However, the rudimentary survival kit contents are mismatched with the intricate realities of forthcoming lunar expedition technology, interactions, and protocols. Therefore, I present a newly re-imagined scenario ‚Äî "How to Survive on the Moon: The Artemis Edition" ‚Äî a mission enriched with exponential technologies and speculated systems, including immersive virtual environments, bio-regenerative life-support, adaptive smart suits, personalised medicine, and 3D additive manufacturing. Today's challenge is again more than updating the toolkit; it primes the augmented human community for the nuanced demands of space and ignites a focus on emergency preparedness. Seeking new ways of collective brainstorming for the reconceptualization of resource management and resourcefulness remains critical. Ultimately, asking important questions about what Augmented Astronauts need, seeks to support the next generation of lunar explorers‚Äîempowered today by global transdisciplinary collaboration, equipped with transformational technologies, and critically prepared to thrive in the Artemis era.
closekeynote-author: |
  <strong> A/Prof. Sarah Jane Pell </strong>
  <strong>Short Bio:</strong>Sarah Jane Pell is an Associate Professor of Future Interfaces at Monash University, Australia. She is also a Chief Investigator of an ARC-funded Discovery Project into the Design supporting Aquatic Performance in Human-Computer Interaction, or WaterHCI, with the Exertion Games Lab. Previously she held a joint adjunct appointment across the Monash Faculty of Engineering, Office of the Engineering Dean, and Monash Faculty of Art, Design and Architecture [MADA], and Artist in Residence of the Monash Immersive Visualisation Platform [MIVP]. Sarah Jane has served as a vice-chair of the International Astronautical Federation (IAF) Technical Committee for the Cultural Utilisation of Space (ITACCUS), and co-chair of the European Space Agency (ESA) Topical Team for Art & Science (ETTAS). She earned her Bachelor of Fine Arts degree at the Victorian College of the Arts (VCA), Melbourne University, AU, her degree of Master in Human Movement, from Victoria University, AU, and graduate certificates in Space Life Sciences, from the International Space University, FR, and Suborbital Mission Operations, Embry-Riddle Aeronautical University, US. She read for her PhD at the Live Art Unit, at Nottingham Trent University, UK, and WAAPA and SOCA at Edith Cowan University, AU. In 2007, MIT Leonardo LABS awarded her PhD work proposing 'Aquabatics as new works of Live Art' as the 'Best PhD Art & Science'. Dr. Pell is also an internationally recognized artist and ADAS occupational diver. Principal/Founder of the Aquabatics Research Team initiative (ARTi) 2002-2012, her practice has redefined human performance underwater. By pioneering aquatic arts and gaining unique insight into subsea habitability, Pell successfully led the 2006 ISU-NASA project Luna Gaia: Closed-Loop Habitat for the Moon. She has participated in, designed, and led international analogue mission simulations including MDRS Crew 188, LunAres III SPECTRA, and MAU NEPAL-01. As an elite athlete, she has won multiple world championships, and combining her mission, vision, and passion, she attempted to summit Mt. Everest as an arts-led space analogue, reaching beyond Base Camp, only to survive the deadly 2015 Gorkha Earthquakes. Pell has served as the undersea simulation astronaut for Project MOONWALK: European Human-Robotic Cooperation Lunar Analogue Trails, Marseille, FR, and has tested IVA Spacesuits in Flight, as a Bioastronautics Researcher, for Integrated Spaceflight Services, US, and Project PoSSUM: Polar Suborbital Science in the Upper Mesosphere. Artworks include launching poetic and technical Space Art Payloads, Moon-Earth-Moon transmissions, and terrestrial mixed media exhibitions, museum artefacts and live or immersive performances. The Australia Council have recognised Pell as an Arts Leader and awarded her the highest individual Fellowship for Performing Astronautics. Associate Professor Pell sees our future living simultaneously underwater and in space by evolving existing biomes in both immersive and expansive ways. International honours include being the first Australian awarded a TED Fellow, US, and Gifted Citizen, Global.
# keynote-short: |
#   <h3 style="color:#003865;">Behavior and Physiology-Aware Security User Interfaces</h3>
# keynote-author-short: |
#   <strong>Florian Alt</strong> is a full professor of Usable Security and Privacy at the Research Institute for Cyber Defence (CODE) in Munich.
closekeynote-img: /img/keynote-speaker/jane.jpeg

# program-papers:
#   - Detecting Episodes of Increased Cough Using Kinetic Earables<br /><small>Tobias R√∂ddiger, Michael Beigl, Michael Hefenbrock, Daniel Wolffram and Erik Pescara</small>
#   - Portable 3D Human Pose Estimation for Human-Human Interaction using a Chest-Mounted Fisheye Camera <br /><small>Kohei Aso, Dong-Hyun Hwang and Hideki Koike</small>
#   - CapGlasses&#58; Untethered Capacitive Sensing with Smart Glasses <br /><small>Denys J.C. Matthies, Chamod Weerasinghe, Suranga Nanayakkara and Bodo Urban</small>
#   - Exploratory Design of a Hands-free Video Game Controller for a Quadriplegic Individual <br /><small>Atieh Taheri and Misha Sra</small>
#   - Virtual Whiskers&#58; Spatial Directional Guidance using Cheek Haptic Stimulation in a Virtual Environment <br /><small>Fumihiko Nakamura, Adrien Verhulst, Kuniharu Sakurada and Maki Sugimoto</small>
#   - Wearable System for Promoting Salivation <br /><small>Kai Washino, Ayumi Ohnishi, Tsutomu Terada and Masahiko Tsukamoto</small>
#   - HemodynamicVR - Adapting the User's Field Of View during Virtual Reality Locomotion Tasks using Wearable Functional Near-Infrared Spectroscopy <br /><small>Hiroo Yamamura, Holger Baldauf and Kai Kunze</small>
#   - Augmented Foot&#58; A Comprehensive Survey of Foot Augmentation Devices <br /><small>Don Samitha Elvitigala, Jochen Huber and Suranga Nanayakkara</small>
#   - Motion-specific Browsing method by mapping to a circle for personal video Observation with Head-Mounted Displays <br /><small>Natsuki Hamanishi and Jun Rekimoto</small>
#   - Exploring Pseudo Hand-Eye Interaction on the Head-Mounted Display <br /><small>Myung Jin Kim and Andrea Bianchi</small>
#   - MultiSoma&#58; Distributed Embodiment with Synchronized Behavior and Perception <br /><small>Reiji Miura, Maki Sugimoto, Shunichi Kasahara, Michiteru Kitazaki, Adrien Verhulst and Masahiko Inami</small>
#   - Dynamic Shared Limbs&#58; An Adaptive Shared Body Control Method Using EMG Sensors <br /><small>Ryo Takizawa, Takayoshi Hagiwara, Adrien Verhulst, Masaaki Fukuoka, Michiteru Kitazaki and Maki Sugimoto</small>
#   - Independent Control of Supernumerary Appendages Exploiting Upper Limb Redundancy <br /><small>Hideki Shimobayashi, Tomoya Sasaki, Arata Horie, Riku Arakawa, Zendai Kashino and Masahiko Inami</small>
#   - Research on the transcendence of bodily differences, using sport and human augmentation medium <br /><small>Ryoichi Ando, Isao Uebayashi, Hayato Sato, Hayato Ohbayashi, Shota Katagiri, Shuhei Hayakawa and Kouta Minamizawa</small>
#   - SilentMask&#58; Mask-type Silent Speech Interface with Measurement of Mouth Movement <br /><small>Hirotaka Hiraki and Jun Rekimoto</small>
#   - Derma&#58; Silent Speech Interaction Using Transcutaneous MotionSensing <br /><small>Jun Rekimoto and Yui Nishimura</small>
#   - Conversational Partner‚Äôs Perception of Subtle Display Use for Monitoring Notifications <br /><small>Jacob Logas, Kelsie Belan, Thad Starner and Blue Lin</small>
#   - Deep Learning-Based Scene Simplification for Bionic Vision <br /><small>Nicole Han, Sudhanshu Srivastava, Aiwen Xu, Devi Klein and Michael Beyeler</small>
#   - FaceRecGlasses&#58; A Wearable System for Recognizing Self Facial Expression Using Compact Wearable Cameras <br /><small>Hiroaki Aoki, Ayumi Ohnishi, Naoya Isoyama, Tsutomu Terada and Masahiko Tsukamoto</small>
#   - CircadianVisor&#58; Image Presentation With an Optical See-Through Display in Consideration of Circadian Illuminance <br /><small>Takumi Tochimoto, Yuichi Hiroi and Yuta Itoh</small>
#   - Advantage and Misuse of Vision Augmentation - Exploring User Perceptions and Attitudes using a Zoom Prototype <br /><small>Chloe Eghtebas, Francisco Kiss, Marion Koelle and Pawe≈Ç Wo≈∫niak</small>
#   - SmartAidView Jacket&#58; Providing visual aid to lower the underestimation of assistive forces <br /><small>Swagata Das, Velika Wongchadakul and Yuichi Kurita</small>
#   - Virtual Physical Task Training&#58; Comparing Shared Body, Shared View and Verbal Task Explanation <br /><small>Jens Reinhardt, Marco Kurzweg and Katrin Wolf</small>
#   - Hippocampal Cognitive Prosthesis, Memory and Identity&#58; Case Study for the Role of Ethics Guidelines for Human Enhancement <br /><small>Yasemin J. Erden and Philip Brey</small>
#   - CV-Based Analysis for Microscopic Gauze Suturing Training <br /><small>Mikihito Matsuura, Shio Miyafuji, Erwin Wu, Satoshi Kiyofuji, Taichi Kin, Takeo Igarashi and Hideki Koike</small>
#   - A Machine Learning Model Perceiving Brightness Optical Illusions&#58; Quantitative Evaluation with Psychophysical Data <br /><small>Yuki Kubota, Atsushi Hiyama and Masahiko Inami</small>
#   - POV Display and Interaction Methods extending Smartphone <br /><small>Yura Tamai, Maho Oki and Koji Tsukada</small>
#   - From Strangers to Friends&#58; Augmenting Face-to-face Interactions with Faceted Digital Self-Presentations <br /><small>Mikko Kyt√∂, Ilyena Hirskyj-Douglas and David McGookin</small>
#   - Interactive Eye Aberration Correction for Holographic Near-Eye Display <br /><small>Kenta Yamamoto, Ippei Suzuki, Kosaku Namikawa, Kaisei Sato and Yoichi Ochiai</small>
#   - Ubiquitous Body&#58; Effect of Spatial Arrangement of Task‚Äôs View on Managing Multiple Tasks<br <br/><small>Yukiko Iwasaki and Hiroyasu Iwata</small>


# keynote-title: Keynotes
# keynotes:

# - speaker: Prof. Thad Starner
#   Title: Augmenting human animal communication
#   image: /img/keynote-speaker/thad_starner_0.png
#   Abstract: Animals perceive much more than they can communicate with humans. For example, service dogs can sense when their handlers are about to fall victim to insulin shock. Bomb sniffing dogs can tell the difference between c4 and a peroxide bomb. Pods of dolphins have complex social structures and even refer to each other by name. Over the past decade, we have investigated how to use computer interfaces to help bridge the gap between animal and human communication. In this talk we will discuss some of the more practical use cases for what we have discovered.
#   Biography: Thad Starner is a Professor at the Georgia Institute of Technology's School of Interactive Computing. Thad was perhaps the first to integrate a wearable computer into his everyday life as an intelligent personal assistant. Starner's work as a PhD student would help found the field of Wearable Computing. 

# - speaker: Pattie Maes
#   Title: TBA
#   image: /img/keynote-speaker/pattie_maes.png
#   Biography: Pattie Maes is a professor in MIT's Program in Media Arts and Sciences and until recently served as academic head. She runs the Media  Lab's Fluid Interfaces research group, which aims to radically reinvent the human-machine experience. Coming from a background in artificial intelligence and human-computer interaction, she is particularly interested in the topic of cognitive enhancement, or how immersive and wearable systems can actively assist people with memory, attention, learning, decision making, communication, and wellbeing. 
